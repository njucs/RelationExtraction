{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njucs/RelationExtraction/blob/main/CRE/ACL2019_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python 2.7 + Tensorflow 1.14.0\n",
        "!python --version\n",
        "!whereis python2\n",
        "#!python -m pip uninstall pip\n",
        "#!update-alternatives --remove python /usr/local/bin/python\n",
        "!update-alternatives --install /usr/local/bin/python python /usr/bin/python2\n",
        "#!wget https://bootstrap.pypa.io/get-pip.py\n",
        "#!python get-pip.py\n",
        "\n",
        "!python --version\n",
        "#!pip install tensorflow-gpu==1.14"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFvaJm6WBk1G",
        "outputId": "f7674345-cf8d-42f9-b123-5460de9d500b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n",
            "python2: /usr/bin/python2.7-config /usr/bin/python2 /usr/bin/python2.7 /usr/lib/python2.7 /etc/python2.7 /usr/local/lib/python2.7 /usr/include/python2.7 /usr/share/man/man1/python2.1.gz\n",
            "update-alternatives: --install needs <link> <name> <path> <priority>\n",
            "\n",
            "Use 'update-alternatives --help' for program usage information.\n",
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGRli8vXAuuX"
      },
      "source": [
        "#### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RvQVbokAuub",
        "outputId": "053928fd-2a63-4029-f204-2f5c32e48ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import cPickle\n",
        "import _pickle as cPickle\n",
        "from numpy import array\n",
        "import operator\n",
        "import random\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "DEVICE = \"0\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=DEVICE\n",
        "\n",
        "INPUT_LENGTH = 32\n",
        "OUTPUT_LENGTH = 14\n",
        "TOTAL_EPOCHS = 20\n",
        "BEAMSEARCH = True\n",
        "POSPROCESSING = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QLdbpHMAuuc"
      },
      "outputs": [],
      "source": [
        "word_to_idx = cPickle.load(open(\"data/ready/encoder-decoder-model-training/source_embedding_dict.pickle\", \"rb\"))\n",
        "#idx_to_word = {v: k for k, v in word_to_idx.iteritems()}\n",
        "idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
        "\n",
        "ent_to_idx = cPickle.load(open(\"data/ready/encoder-decoder-model-training/target_embedding_dict.pickle\", \"rb\"))\n",
        "#idx_to_ent = {v: k for k, v in ent_to_idx.iteritems()}\n",
        "idx_to_ent = {v: k for k, v in ent_to_idx.items()}\n",
        "\n",
        "with open(\"data/ready/encoder-decoder-model-training/source_embedding_matrix.pickle\", \"rb\") as f:\n",
        "    source_embedding_matrix = cPickle.load(f, encoding=\"bytes\")\n",
        "\n",
        "with open(\"data/ready/encoder-decoder-model-training/target_embedding_matrix.pickle\", \"rb\") as f:\n",
        "    target_embedding_matrix = cPickle.load(f, encoding=\"bytes\")\n",
        "    \n",
        "#source_embedding_matrix = cPickle.load(open(\"data/ready/encoder-decoder-model-training/source_embedding_matrix.pickle\", \"rb\"))\n",
        "#target_embedding_matrix = cPickle.load(open(\"data/ready/encoder-decoder-model-training/target_embedding_matrix.pickle\", \"rb\"))\n",
        "\n",
        "train_dataset = cPickle.load(open(\"data/ready/encoder-decoder-model-training/train_dataset.pickle\", \"rb\"))\n",
        "test_dataset = cPickle.load(open(\"data/ready/encoder-decoder-model-training/test_dataset.pickle\", \"rb\"))\n",
        "val_dataset = cPickle.load(open(\"data/ready/encoder-decoder-model-training/val_dataset.pickle\", \"rb\"))\n",
        "trip_dataset = cPickle.load(open(\"data/ready/encoder-decoder-model-training/trip_dataset.pickle\", \"rb\"))\n",
        "\n",
        "pred_dict = cPickle.load(open(\"data/ready/pred_to_idx.pickle\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_smubXKAuud"
      },
      "outputs": [],
      "source": [
        "with open(\"data/ready/dictionary.pickle\", \"rb\") as f:\n",
        "    dictionary = cPickle.load(f, encoding=\"bytes\")\n",
        "\n",
        "with open(\"data/ready/wikidata_link.pickle\", \"rb\") as f:\n",
        "    wikidata_to_dbp = cPickle.load(f, encoding=\"bytes\")\n",
        "\n",
        "#dictionary = cPickle.load(open(\"data/ready/dictionary.pickle\",\"rb\"))\n",
        "#wikidata_to_dbp = cPickle.load(open(\"data/ready/wikidata_link.pickle\",\"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GUaMa_g4Auue",
        "outputId": "82e8a3e4-8b35-4d7a-e061-b8ead179b214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training size 224881\n",
            "validation size 988\n",
            "test size 29785\n",
            "trip size 1000\n"
          ]
        }
      ],
      "source": [
        "training_input = list()\n",
        "training_output = list()\n",
        "for t in train_dataset:\n",
        "    input_data, output_data, ner_data = t\n",
        "    \n",
        "    input_data = input_data.split()\n",
        "    input_data = [word_to_idx[x] for x in input_data]\n",
        "    input_data.insert(0, word_to_idx[\"<START>\"])\n",
        "    input_data.append(word_to_idx[\"<END>\"])\n",
        "    sent_array = [0] * INPUT_LENGTH\n",
        "    sent_array[:len(input_data)] = input_data\n",
        "    training_input.append(sent_array)\n",
        "    \n",
        "    output_data = [ent_to_idx[x] for x in output_data]\n",
        "    output_data.insert(0, ent_to_idx[\"<START>\"])\n",
        "    output_data.append(ent_to_idx[\"<END>\"])\n",
        "    target_array = [0] * OUTPUT_LENGTH\n",
        "    target_array[:len(output_data)] = output_data\n",
        "    training_output.append(target_array)\n",
        "    \n",
        "testing_input = list()\n",
        "testing_output = list()\n",
        "for t in test_dataset:\n",
        "    input_data, output_data, ner_data = t\n",
        "    \n",
        "    input_data = input_data.split()\n",
        "    input_data = [word_to_idx[x] for x in input_data]\n",
        "    input_data.insert(0, word_to_idx[\"<START>\"])\n",
        "    input_data.append(word_to_idx[\"<END>\"])\n",
        "    sent_array = [0] * INPUT_LENGTH\n",
        "    sent_array[:len(input_data)] = input_data\n",
        "    testing_input.append(sent_array)\n",
        "    \n",
        "    output_data = [ent_to_idx[x] for x in output_data]\n",
        "    output_data.insert(0, ent_to_idx[\"<START>\"])\n",
        "    output_data.append(ent_to_idx[\"<END>\"])\n",
        "    target_array = [0] * OUTPUT_LENGTH\n",
        "    target_array[:len(output_data)] = output_data\n",
        "    testing_output.append(target_array)\n",
        "    \n",
        "    \n",
        "validation_input = list()\n",
        "validation_output = list()\n",
        "for t in val_dataset:\n",
        "    input_data, output_data, ner_data = t\n",
        "    \n",
        "    input_data = input_data.split()\n",
        "    input_data = [word_to_idx[x] for x in input_data]\n",
        "    input_data.insert(0, word_to_idx[\"<START>\"])\n",
        "    input_data.append(word_to_idx[\"<END>\"])\n",
        "    sent_array = [0] * INPUT_LENGTH\n",
        "    sent_array[:len(input_data)] = input_data\n",
        "    validation_input.append(sent_array)\n",
        "    \n",
        "    output_data = [ent_to_idx[x] for x in output_data]\n",
        "    output_data.insert(0, ent_to_idx[\"<START>\"])\n",
        "    output_data.append(ent_to_idx[\"<END>\"])\n",
        "    target_array = [0] * OUTPUT_LENGTH\n",
        "    target_array[:len(output_data)] = output_data\n",
        "    validation_output.append(target_array)\n",
        "    \n",
        "trip_input = list()\n",
        "trip_output = list()\n",
        "for t in trip_dataset:\n",
        "    input_data, output_data, ner_data = t\n",
        "    \n",
        "    input_data = input_data.split()\n",
        "    input_data = [word_to_idx[x] for x in input_data]\n",
        "    input_data.insert(0, word_to_idx[\"<START>\"])\n",
        "    input_data.append(word_to_idx[\"<END>\"])\n",
        "    sent_array = [0] * INPUT_LENGTH\n",
        "    sent_array[:len(input_data)] = input_data\n",
        "    trip_input.append(sent_array)\n",
        "    \n",
        "    output_data = [ent_to_idx[x] for x in output_data]\n",
        "    output_data.insert(0, ent_to_idx[\"<START>\"])\n",
        "    output_data.append(ent_to_idx[\"<END>\"])\n",
        "    target_array = [0] * OUTPUT_LENGTH\n",
        "    target_array[:len(output_data)] = output_data\n",
        "    trip_output.append(target_array)\n",
        "    \n",
        "# Name Expansion\n",
        "name_training_input = list()\n",
        "name_training_output = list()\n",
        "for e in ent_to_idx:\n",
        "    if e in wikidata_to_dbp:\n",
        "        ent_name = wikidata_to_dbp[e].split(\"/\")[-1].replace(\"_\", \" \").split()\n",
        "        in_embedding = True\n",
        "        for token in ent_name:\n",
        "            if token not in word_to_idx:\n",
        "                in_embedding = False\n",
        "        if not in_embedding:\n",
        "            in_embedding = True\n",
        "            ent_name = dictionary[e].split()\n",
        "            for token in ent_name:\n",
        "                if token not in word_to_idx:\n",
        "                    in_embedding = False\n",
        "        if in_embedding:\n",
        "            source = list()\n",
        "            source.append(word_to_idx[\"<START>\"])\n",
        "            for token in ent_name:\n",
        "                source.append(word_to_idx[token])\n",
        "            source.append(word_to_idx[\"<END>\"])\n",
        "            target = [ent_to_idx[\"<START>\"], ent_to_idx[e], ent_to_idx[\"<END>\"]]\n",
        "            source[len(source):INPUT_LENGTH] = [0] * (INPUT_LENGTH-len(source))\n",
        "            target[len(target):OUTPUT_LENGTH] = [0] * (OUTPUT_LENGTH-len(target))\n",
        "            name_training_input.append(source)\n",
        "            name_training_output.append(target)\n",
        "    else:\n",
        "        try:\n",
        "            ent_name = dictionary[e].split()\n",
        "            in_embedding = True\n",
        "            for token in ent_name:\n",
        "                if token not in word_to_idx:\n",
        "                    in_embedding = False\n",
        "            if in_embedding:\n",
        "                source = list()\n",
        "                source.append(word_to_idx[\"<START>\"])\n",
        "                for token in ent_name:\n",
        "                    source.append(word_to_idx[token])\n",
        "                source.append(word_to_idx[\"<END>\"])\n",
        "                target = [ent_to_idx[\"<START>\"], ent_to_idx[e], ent_to_idx[\"<END>\"]]\n",
        "                source[len(source):INPUT_LENGTH] = [0] * (INPUT_LENGTH-len(source))\n",
        "                target[len(target):OUTPUT_LENGTH] = [0] * (OUTPUT_LENGTH-len(target))\n",
        "                name_training_input.append(source)\n",
        "                name_training_output.append(target)\n",
        "        except:\n",
        "            pass\n",
        "name_training_input += training_input\n",
        "name_training_output += training_output\n",
        "training_input = name_training_input\n",
        "training_output = name_training_output\n",
        "\n",
        "\n",
        "training_input = array(training_input, dtype=np.int32)\n",
        "training_output = array(training_output, dtype=np.int32)\n",
        "testing_input = array(testing_input, dtype=np.int32)\n",
        "testing_output = array(testing_output, dtype=np.int32)\n",
        "validation_input = array(validation_input, dtype=np.int32)\n",
        "validation_output = array(validation_output, dtype=np.int32)\n",
        "trip_input = array(trip_input, dtype=np.int32)\n",
        "trip_output = array(trip_output, dtype=np.int32)\n",
        "\n",
        "\n",
        "print('training size', len(training_input))\n",
        "print('validation size', len(validation_input))\n",
        "print('test size', len(testing_input))\n",
        "print('trip size', len(trip_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZYFiZ0RAuuh"
      },
      "source": [
        "#### Build MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HloLIncVAuuh",
        "outputId": "e3613d5f-a8df-47bb-fdc4-0decafa9b000"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'contrib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-cdaabfbc259c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_and_drop_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_and_drop_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
          ]
        }
      ],
      "source": [
        "BUFFER_SIZE = len(training_input)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 64\n",
        "units = 512\n",
        "vocab_inp_size = len(word_to_idx)\n",
        "vocab_tar_size = len(ent_to_idx)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((training_input, training_output)).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((validation_input, validation_output)).shuffle(BUFFER_SIZE)\n",
        "val_dataset = val_dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ECubOE6WAuui"
      },
      "outputs": [],
      "source": [
        "def gru(units):\n",
        "    if tf.test.is_gpu_available():\n",
        "        return tf.keras.layers.CuDNNGRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "        return tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_activation='sigmoid', recurrent_initializer='glorot_uniform')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BePgDFCbAuuj"
      },
      "outputs": [],
      "source": [
        "#### ENCODER\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embeddings_initializer = tf.constant_initializer(source_embedding_matrix)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True, trainable=True)\n",
        "        \n",
        "        self.gru = gru(self.enc_units)\n",
        "        \n",
        "    def get_ngram_tensor(self, stacked_tensor, N):\n",
        "        unstacked_tensor = tf.unstack(stacked_tensor, stacked_tensor.get_shape().as_list()[1], 1)\n",
        "        result = list()\n",
        "        for i in range(len(unstacked_tensor)-(N-1)):\n",
        "            tmp_tensor = unstacked_tensor[i]\n",
        "            for j in range(1, N):\n",
        "                tmp_tensor = tf.concat([tmp_tensor, unstacked_tensor[j+i]], 1)\n",
        "            result.append(tmp_tensor)\n",
        "        result = tf.stack(result, 1)\n",
        "        return result\n",
        "    \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        \n",
        "        embedded_input = x\n",
        "        return output, state, embedded_input\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rzPTumn7Auuj"
      },
      "outputs": [],
      "source": [
        "#### DECODER\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embeddings_initializer = tf.constant_initializer(target_embedding_matrix)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True, trainable=True)\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "        self.W1_1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V_1 = tf.keras.layers.Dense(1)\n",
        "        \n",
        "        self.W1_2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V_2 = tf.keras.layers.Dense(1)\n",
        "        \n",
        "        self.W1_3 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V_3 = tf.keras.layers.Dense(1)\n",
        "        \n",
        "        self.W_cv1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W_cv2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W_cv3 = tf.keras.layers.Dense(self.dec_units)\n",
        "        \n",
        "    def get_ngram_tensor(self, stacked_tensor, N):\n",
        "        unstacked_tensor = tf.unstack(stacked_tensor, stacked_tensor.get_shape().as_list()[1], 1)\n",
        "        result = list()\n",
        "        for i in range(len(unstacked_tensor)-(N-1)):\n",
        "            tmp_tensor = unstacked_tensor[i]\n",
        "            for j in range(1, N):\n",
        "                tmp_tensor = tf.concat([tmp_tensor, unstacked_tensor[j+i]], 1)\n",
        "            result.append(tmp_tensor)\n",
        "        result = tf.stack(result, 1)\n",
        "        return result\n",
        "        \n",
        "    def call(self, x, hidden, enc_output, embedded_input):\n",
        "        \n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        enc_output_1 = self.get_ngram_tensor(embedded_input, 1)\n",
        "        score_1 = self.V_1(tf.nn.tanh(self.W1_1(enc_output_1) + self.W2(hidden_with_time_axis)))\n",
        "        attention_weights_1 = tf.nn.softmax(score_1, axis=1)\n",
        "        context_vector_1 = attention_weights_1 * enc_output_1\n",
        "        context_vector_1 = tf.reduce_sum(context_vector_1, axis=1)\n",
        "        context_vector_1 = self.W_cv1(context_vector_1)\n",
        "\n",
        "        enc_output_2 = self.get_ngram_tensor(embedded_input, 2)\n",
        "        score_2 = self.V_2(tf.nn.tanh(self.W1_2(enc_output_2) + self.W2(hidden_with_time_axis)))\n",
        "        attention_weights_2 = tf.nn.softmax(score_2, axis=1)\n",
        "        context_vector_2 = attention_weights_2 * enc_output_2\n",
        "        context_vector_2 = tf.reduce_sum(context_vector_2, axis=1)\n",
        "        context_vector_2 = self.W_cv2(context_vector_2)\n",
        "\n",
        "        enc_output_3 = self.get_ngram_tensor(embedded_input, 3)\n",
        "        score_3 = self.V_3(tf.nn.tanh(self.W1_3(enc_output_3) + self.W2(hidden_with_time_axis)))\n",
        "        attention_weights_3 = tf.nn.softmax(score_3, axis=1)\n",
        "        context_vector_3 = attention_weights_3 * enc_output_3\n",
        "        context_vector_3 = tf.reduce_sum(context_vector_3, axis=1)\n",
        "        context_vector_3 = self.W_cv3(context_vector_3)\n",
        "\n",
        "        context_vector_n = tf.nn.tanh(context_vector_1 + context_vector_2 + context_vector_3)\n",
        "\n",
        "        x = tf.concat([tf.expand_dims(context_vector_n, 1), tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tn-qNzTXAuuk"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pJr0SPNPAuuk"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.train.AdamOptimizer(0.0002)\n",
        "def loss_function(real, pred):\n",
        "    mask = 1 - np.equal(real, 0)\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ftvnt7DoAuuk"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = 'data/ready/encoder-decoder-model-training/model/'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "0LPI5xKFAuul"
      },
      "outputs": [],
      "source": [
        "EPOCHS = TOTAL_EPOCHS\n",
        "\n",
        "log_file = open(\"log\", \"w\", 0)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset):\n",
        "        hidden = encoder.initialize_hidden_state()\n",
        "        loss = 0\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden, embedded_input = encoder(inp, hidden)\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_input = tf.expand_dims([word_to_idx['<START>']] * BATCH_SIZE, 1)       \n",
        "\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output, embedded_input)\n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        total_loss += batch_loss\n",
        "        variables = encoder.variables + decoder.variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            log_str = ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "            print log_str\n",
        "            log_file.write(log_str+\"\\n\")\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    val_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(val_dataset):\n",
        "        hidden = encoder.initialize_hidden_state()\n",
        "        loss = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden, embedded_input = encoder(inp, hidden)\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_input = tf.expand_dims([word_to_idx['<START>']] * BATCH_SIZE, 1)       \n",
        "\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output, embedded_input)\n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        val_loss += batch_loss\n",
        "\n",
        "    log_str = ('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / N_BATCH))\n",
        "    print log_str\n",
        "    log_file.write(log_str+\"\\n\")\n",
        "    log_str = ('Epoch {} Val Loss {:.4f}'.format(epoch + 1, val_loss / (len(validation_input)//BATCH_SIZE)))\n",
        "    print log_str\n",
        "    log_file.write(log_str+\"\\n\")\n",
        "    log_str = ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    print log_str\n",
        "    log_file.write(log_str+\"\\n\")\n",
        "print \"DONE\"\n",
        "log_file.write(\"DONE\\n\")\n",
        "log_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ni6qGsGAuul"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKJKIg_iAuul"
      },
      "outputs": [],
      "source": [
        "# LOAD EXISTING MODEL\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xk8ghAr3Auum"
      },
      "outputs": [],
      "source": [
        "def get_gold_standard(expected_output):\n",
        "    result = []\n",
        "    for e in expected_output:\n",
        "        result.append(idx_to_ent[e])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqrZc7tAAuum"
      },
      "outputs": [],
      "source": [
        "clf = cPickle.load(open(\"data/ready/triple_classifier_model/model.pickle\", \"rb\"))\n",
        "\n",
        "def one_hot_encoding(pred, pred_dict):\n",
        "    enc = [0]*len(pred_dict)\n",
        "    enc[pred_dict[pred]] = 1\n",
        "    return enc\n",
        "\n",
        "def get_sent(result, prob_result, postprocessing):\n",
        "    sent_set = set()\n",
        "    sent = list()\n",
        "    sent_id = list()\n",
        "    for i,token in enumerate(result):\n",
        "        if token == \"<START>\":\n",
        "            continue\n",
        "        if token == \"<END>\":\n",
        "            if len(sent) == 3:\n",
        "                sent_set.add(\" ### \".join(sent))\n",
        "            return sent_set\n",
        "        sent.append(dictionary[token])\n",
        "        sent_id.append(token)\n",
        "        if len(sent) == 3:\n",
        "            if postprocessing:\n",
        "                if (prob_result[i]+prob_result[i-1]+prob_result[i-2])/3 < 0.70:\n",
        "                    s = sent_id[0]\n",
        "                    p = sent_id[1]\n",
        "                    o = sent_id[2]\n",
        "\n",
        "                    if s.startswith(\"Q\") and p.startswith(\"P\") and o.startswith(\"Q\"):\n",
        "\n",
        "                        embedding_s = np.asarray(target_embedding_matrix[ent_to_idx[s]]).astype(np.float)\n",
        "                        embedding_o = np.asarray(target_embedding_matrix[ent_to_idx[o]]).astype(np.float)\n",
        "                        embedding_p = np.asarray(target_embedding_matrix[ent_to_idx[p]]).astype(np.float)\n",
        "                        diff = (abs(embedding_s+embedding_p-embedding_o)).sum()\n",
        "                        valid_triple = clf.predict([one_hot_encoding(p, pred_dict)+ one_hot_encoding(TEST_DATASET, {\"seen\":0,\"nyt\":1,\"trip\":2})+[diff]])[0]\n",
        "                        if valid_triple == 0:\n",
        "                            sent_set.add(\" ### \".join(sent))\n",
        "                else:\n",
        "                    sent_set.add(\" ### \".join(sent))\n",
        "            else:\n",
        "                sent_set.add(\" ### \".join(sent))\n",
        "            sent = list()\n",
        "    return sent_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mi2S4EgAAuum"
      },
      "outputs": [],
      "source": [
        "def get_sent_from_training_data(training_data):\n",
        "    sent = \"\"\n",
        "    for tok in training_data:\n",
        "        sent += idx_to_word[tok]+\" \"\n",
        "        if idx_to_word[tok] == \"<END>\":\n",
        "            return sent.strip() \n",
        "    return sent.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qsHrz8p0Auum"
      },
      "outputs": [],
      "source": [
        "from kitchen.text.converters import getwriter, to_bytes, to_unicode\n",
        "from kitchen.i18n import get_translation_object\n",
        "translations = get_translation_object('example')\n",
        "_ = translations.ugettext\n",
        "b_ = translations.lgettext\n",
        "\n",
        "def phrase_sim(sent, phrases):\n",
        "    result = list()\n",
        "    pos = 0\n",
        "    for p in phrases:\n",
        "        l = len(p.split())\n",
        "        splitted = sent.split()\n",
        "        max_score = 0\n",
        "        for i in range(len(splitted)-l+1):\n",
        "            test = \" \".join(splitted[i:i+l])\n",
        "            score = fuzz.ratio(p, test)\n",
        "            if score > max_score:\n",
        "                max_score = score\n",
        "        result.append((p, len(phrases)-pos, max_score))\n",
        "        pos += 1\n",
        "    result = sorted(result, key = lambda x: (x[2], x[1],len(x[0])), reverse=True)\n",
        "    return result\n",
        "\n",
        "def generate_output(sentence, encoder, decoder):\n",
        "    attention_plot = np.zeros((OUTPUT_LENGTH, INPUT_LENGTH))\n",
        "    sentence_ori = get_sent_from_training_data(sentence[0])\n",
        "    result = list()\n",
        "    prob_result = list()\n",
        "    inputs = tf.convert_to_tensor(sentence)\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden, embedded_input = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([ent_to_idx['<START>']], 0)\n",
        "\n",
        "    if BEAMSEARCH:\n",
        "        for t in range(OUTPUT_LENGTH):\n",
        "            if t%3==0 or t%3==2:\n",
        "                beam = True\n",
        "            else:\n",
        "                beam = False\n",
        "            predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out, embedded_input)\n",
        "            prob_predictions = tf.nn.softmax(predictions)\n",
        "            attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "            attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "            if prob_predictions[0][predicted_id] > 0.99:\n",
        "                beam = False\n",
        "            \n",
        "            if idx_to_ent[predicted_id] == '<END>':\n",
        "                return result,prob_result,sentence_ori, attention_plot\n",
        "\n",
        "            if beam:\n",
        "                if t%3==2:\n",
        "                    emb_s = np.asarray(target_embedding_matrix[ent_to_idx[result[t-2]]])\n",
        "                    emb_p = np.asarray(target_embedding_matrix[ent_to_idx[result[t-1]]])\n",
        "                    diff = tf.cast(tf.nn.softmax(-np.sum(abs(emb_s+emb_p-target_embedding_matrix), axis=1), axis=0), tf.float32)\n",
        "                    prob_predictions_tmp = tf.multiply(prob_predictions[0], diff)\n",
        "                    cand_predicted_id = tf.nn.top_k(prob_predictions_tmp, k=100, sorted=True)[1].numpy()\n",
        "                else:\n",
        "                    cand_predicted_id = tf.nn.top_k(predictions[0], k=10, sorted=True)[1].numpy()\n",
        "                \n",
        "                phrases = list()\n",
        "                rev_dict = dict()\n",
        "                for p in cand_predicted_id:\n",
        "                    if idx_to_ent[p] == \"<END>\" or idx_to_ent[p] == \"<START>\":\n",
        "                        continue\n",
        "                    phrases.append(_(dictionary[idx_to_ent[p]]))\n",
        "                    rev_dict[dictionary[idx_to_ent[p]]] = idx_to_ent[p]\n",
        "                predicted_id = np.int64(ent_to_idx[rev_dict[b_(phrase_sim(sentence_ori, phrases)[0][0])]])\n",
        "            else:\n",
        "                predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "                \n",
        "            result.append(idx_to_ent[predicted_id])\n",
        "            prob_result.append(prob_predictions[0][predicted_id])\n",
        "            dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    else:\n",
        "        for t in range(OUTPUT_LENGTH):\n",
        "            predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out, embedded_input)\n",
        "            prob_predictions = tf.nn.softmax(predictions)\n",
        "            attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "            attention_plot[t] = attention_weights.numpy()\n",
        "            \n",
        "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "            if idx_to_ent[predicted_id] == '<END>':\n",
        "                return result,prob_result,sentence_ori, attention_plot\n",
        "            \n",
        "            result.append(idx_to_ent[predicted_id])\n",
        "            prob_result.append(prob_predictions[0][predicted_id])\n",
        "            dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result,prob_result,sentence_ori, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VkvoKBXPAuun"
      },
      "outputs": [],
      "source": [
        "INSPECT = True\n",
        "TEST_DATASET = \"seen\" # 'trip' or 'seen'\n",
        "\n",
        "log_file = open(\"result_log\", \"w\", 0)\n",
        "\n",
        "if TEST_DATASET == \"nyt\":\n",
        "    test_in = nyt_input\n",
        "    test_out = nyt_output\n",
        "elif TEST_DATASET == \"seen\":\n",
        "    test_in = testing_input\n",
        "    test_out = testing_output\n",
        "elif TEST_DATASET == \"trip\":\n",
        "    test_in = trip_input\n",
        "    test_out = trip_output\n",
        "\n",
        "if INSPECT:\n",
        "    len_test = 100\n",
        "else:\n",
        "    len_test = len(test_in)\n",
        "\n",
        "num_correct = 0\n",
        "total_prediction = 0\n",
        "total_gold_standard = 0\n",
        "eval_dataset = (test_in[:len_test], test_out[:len_test])\n",
        "for i, test_sent in enumerate(eval_dataset[0]):\n",
        "    if (i+1) % 100 == 0:\n",
        "        print i,str(num_correct/float(total_prediction)),str(num_correct/float(total_gold_standard))\n",
        "        log_file.write(str(i)+\" \"+str(num_correct/float(total_prediction))+\" \"+str(num_correct/float(total_gold_standard))+\"\\n\")\n",
        "    result,prob_result,sentence, attention_plot = generate_output([test_sent], encoder, decoder)\n",
        "    result_sent = get_sent(result[:], prob_result, POSPROCESSING)\n",
        "    gold_standard = get_gold_standard(eval_dataset[1][i])\n",
        "    gold_standard_sent = get_sent(gold_standard,None, False)\n",
        "\n",
        "\n",
        "    if INSPECT:\n",
        "        if result_sent != gold_standard_sent:\n",
        "            print i\n",
        "            print \"Input: \", sentence\n",
        "            print \"Predicted: \", result, result_sent\n",
        "            print \"Expected: \", gold_standard, gold_standard_sent\n",
        "            print\n",
        "        if i > 100:\n",
        "            break\n",
        "\n",
        "    total_prediction += len(result_sent)\n",
        "    total_gold_standard += len(gold_standard_sent)\n",
        "\n",
        "    for s in result_sent:\n",
        "        if s in gold_standard_sent:\n",
        "            num_correct += 1\n",
        "print \"PRECISION: \", num_correct/float(total_prediction)\n",
        "print \"RECALL: \", num_correct/float(total_gold_standard)\n",
        "log_file.write(\"PRECISION: \"+str(num_correct/float(total_prediction))+\"\\n\")\n",
        "log_file.write(\"RECALL: \"+str(num_correct/float(total_gold_standard))+\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "ACL2019_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}